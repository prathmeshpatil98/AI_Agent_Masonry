

### **Web Research Agent - Detailed Plan**

#### 1. **Agent's Core Objective**
The purpose of the Web Research Agent is to **automatically** search the web, **extract relevant information** and **create a comprehensive research report** based on a user‚Äôs query. It uses AI and various tools to gather data from websites, analyze it and summarize the findings into a clear report.

---

### **How the Agent Works - Step-by-Step:**

#### **Step 1: Understand the User's Query**
- **What happens**: The user types a question or a research topic into the app.
- **What the agent does**: It analyzes the query to understand what the user is asking. For example, if the user asks ‚ÄúWhat are the benefits of meditation?‚Äù, the agent understands that it needs to look for **benefits of meditation** across different sources.

#### **Step 2: Search the Web for Relevant Information**
- **What happens**: The agent performs a web search using Google (or other search engines). It looks for web pages, articles, blogs, or other content that are related to the topic.
- **What the agent does**: It gathers **a list of search results** that look promising based on the query.

#### **Step 3: Extract Text from Relevant Websites**
- **What happens**: After finding relevant websites, the agent opens each page and extracts the **main content** from them. This means it pulls out the **text** of the articles, such as the body paragraphs, summaries, or detailed explanations.
- **What the agent does**: It looks for important and useful data on these pages, such as key facts, quotes, and statistics.

#### **Step 4: Analyze the Extracted Data**
- **What happens**: The agent checks whether the extracted information is relevant to the user‚Äôs query.
- **What the agent does**: It scores the relevance of the content. If a web page contains useful information related to the query, it moves forward with analyzing the page; if not, it skips that page.

#### **Step 5: Fetch Latest News (Optional)**
- **What happens**: If applicable, the agent searches for the latest **news articles** related to the topic to make sure the information is up-to-date.
- **What the agent does**: It uses a news API (like NewsAPI) to get recent news articles and includes them in the final report if relevant.

#### **Step 6: Summarize the Findings**
- **What happens**: Once the agent has enough data from websites and news, it needs to summarize the findings.
- **What the agent does**: The agent uses AI (in this case, **Google Gemini**) to generate a **summary** of the research, pulling together key findings, facts, and sources. It presents the results in a readable, well-structured format.

#### **Step 7: Deliver the Final Report**
- **What happens**: The agent presents the research findings back to the user in a report format.
- **What the agent does**: The final report includes:
  - Key points and findings
  - A summary of relevant data, timelines or statistics
  - Citations to sources where the information was gathered.

---

### **Flowchart - How the Information Moves Through the Agent**

Here‚Äôs a simplified flow of how the agent handles a user‚Äôs request:

```
[User enters a query] 
       |
       v
[Step 1: Analyze the query]
       |
       v
[Step 2: Perform Web Search]  -----> [Search Results]
       |
       v
[Step 3: Extract content from web pages]
       |
       v
[Step 4: Analyze extracted data for relevance]
       |
       v
[Step 5: Fetch Latest News (if needed)]
       |
       v
[Step 6: Summarize findings with AI]
       |
       v
[Step 7: Deliver Final Research Report]
```

---

### **Step by Step Decision Making Process**

1. **Analyze the Query**:
   - First, the agent looks at the user's query and figures out what information is needed.
   - Example: ‚ÄúWhat is climate change?‚Äù means it should search for **scientific studies, news and expert opinions on climate change**.

2. **Determine Search Terms**:
   - Based on the query, the agent figures out the most relevant **keywords** or **phrases** to search.
   - Example: For "climate change," keywords might include ‚Äúglobal warming,‚Äù ‚Äúenvironment,‚Äù ‚Äúgreenhouse gases,‚Äù etc.

3. **Search the Web**:
   - The agent uses these keywords to search the web for relevant content.
   - Example: It finds articles, blogs, research papers, and news stories that contain information about climate change.

4. **Extract Data from Web Pages**:
   - For each search result, the agent visits the link and pulls out the **main text** from the web page (ignoring ads, sidebars, etc.).
   - Example: The agent finds an article that explains the causes of climate change in detail and extracts this text.

5. **Analyze Content for Relevance**:
   - The agent checks if the extracted text is useful by comparing it against the user‚Äôs query.
   - If it‚Äôs relevant, the agent moves forward with the data. If not, it skips that page.
   
6. **Fetch Latest News (Optional)**:
   - If the user needs up-to-date information, the agent will check news sources.
   - Example: The agent may find recent articles about new climate change policies or scientific discoveries.

7. **Generate the Final Summary**:
   - Using AI, the agent combines all the data it has collected into a coherent **research report**.
   - It will summarize key findings, list facts and statistics, and include links to sources.

8. **Deliver the Final Report**:
   - The agent presents the findings to the user in a **well-organized, readable format**.

---

### **How the Agent Handles Problems**

- **Problem 1: Unreachable Website**
   - If a website is down or unreachable, the agent will **skip that website** and move on to the next one.
   - If no websites are available, it will inform the user: ‚ÄúWe couldn‚Äôt reach some websites, but we still found useful information.‚Äù

- **Problem 2: Conflicting Information**
   - If different sources give conflicting information, the agent will **highlight** the discrepancy and provide a balanced view.
   - Example: If one website says ‚ÄúClimate change is caused by human activities,‚Äù and another says ‚ÄúClimate change is a natural process,‚Äù the agent will explain both viewpoints and note that there‚Äôs debate among experts.

- **Problem 3: No Relevant Information**
   - If the agent doesn‚Äôt find enough relevant content, it will tell the user: ‚ÄúWe couldn‚Äôt find much relevant information on this topic. You might want to try refining your query or try a different approach.‚Äù

- **Problem 4: Slow Response or API Failure**
   - If the news API or web scraper fails, the agent will **log the error** and still provide a report, possibly using fewer sources.
   - The agent will also notify the user: ‚ÄúWe experienced an issue fetching some information. The report might be incomplete.‚Äù

---

### **Summary of Key Handling Steps**
- **For Web Research**: The agent searches for relevant content, analyzes it, and filters out irrelevant or incorrect data.
- **For Errors**: The agent tries to handle issues like slow websites or missing data gracefully by either skipping problematic sources or notifying the user.

---






---

## üß∞ TOOLS Integration OVERVIEW

### 1. `web_search_tool.py`
**Purpose:** Search the web for relevant links and snippets.

- **Input:**  
  `query` ‚Üí the user‚Äôs research topic (e.g., "climate change impact in 2024")
  
- **Output:**  
  A list of dictionaries like:  
  ```python
  [
    {
      "title": "Global Warming Trends",
      "link": "https://example.com/article",
      "snippet": "A recent study shows..."
    },
    ...
  ]
  ```

- **How the agent uses it:**  
  Provides the **first batch of URLs** and preview texts. These links become the **primary content sources** to be scraped, analyzed, and possibly used in the final report.

---

### 2. `web_scraper_tool.py`
**Purpose:** Extract the full text content from a web page.

- **Input:**  
  A URL from search results

- **Output:**  
  Raw body text of the webpage (up to ~3000 characters)

- **How the agent uses it:**  
  Provides **full textual content** from links. This is passed to the analyzer to check for relevance and extract summaries.

---

### 3. `content_analyzer_tool.py`
**Purpose:** Determine if a scraped page is relevant to the query and summarize it.

- **Input:**  
  - `raw_text`: full web page text from scraper  
  - `keywords`: list of keywords extracted from user query

- **Output:**  
  ```python
  {
    "relevance_score": 4,
    "relevant": True,
    "summary": "This article discusses..."
  }
  ```

- **How the agent uses it:**  
  It **filters out irrelevant pages** and **summarizes only relevant sources** for inclusion in the report.

---

### 4. `news_aggregator_tool.py`
**Purpose:** Fetch recent news articles related to the topic.

- **Input:**  
  - `query`: same user research query  
  - `page_size`: how many articles to fetch (default is 5)

- **Output:**  
  A list of news article dictionaries:  
  ```python
  [
    {
      "headline": "Climate report 2024",
      "summary": "The new UN climate report...",
      "url": "https://news.com/article",
      "source": "Reuters"
    },
    ...
  ]
  ```

- **How the agent uses it:**  
  Used to **add current and credible context** to the report. It enhances trust and timeliness.

---

### 5. `llm_connector.py` (`query_gemini`)
**Purpose:** Generate a human-like summary from the collected data.

- **Input:**  
  A structured prompt including:
  - The user query
  - News summaries
  - Relevant analyzed content

- **Output:**  
  A full-length research report written by Gemini (text)

- **How the agent uses it:**  
  Final step ‚Äî Gemini compiles everything into a **well-structured, cited, human-readable report** for the end user.

---

## üß† How the Agent Uses These Tools Together

The flow looks like this:

```
User Query
   ‚Üì
[web_search_tool] ‚Üí get links
   ‚Üì
[web_scraper_tool] ‚Üí get full text
   ‚Üì
[content_analyzer_tool] ‚Üí check for relevance, extract summaries
   ‚Üì
[news_aggregator_tool] ‚Üí get current articles
   ‚Üì
Compile all sections ‚Üí formatted prompt
   ‚Üì
[llm_connector] ‚Üí send to Gemini ‚Üí get final report
   ‚Üì
Display to user via Streamlit
```
